import { ConvexError } from "convex/values";
import type { ActionCtx } from "../../_generated/server";
import { log } from "../logger";
import type { ApiMessageDoc, ProcessedChunk } from "./types";
import { api } from "../../_generated/api";

// Hierarchical summarization configuration
export const CONTEXT_CONFIG = {
  CHUNK_SIZE: 15, // Messages per chunk
  SUMMARY_THRESHOLD: 20, // When to start summarizing
  MAX_SUMMARY_LENGTH: 400, // Max characters per summary (increased for richer summaries)
  MAX_SUMMARY_CHUNKS: 5, // Max summaries before creating meta-summary
  MIN_CHUNK_SIZE: 10, // Minimum chunk size
  MAX_CHUNK_SIZE: 25, // Maximum chunk size
  // LLM summarization settings
  MAX_API_TOKENS: 1000, // Maximum tokens for API calls
  TEMPERATURE: 0.2, // Temperature for consistent summaries
  TOP_P: 0.9, // Top-p for focused generation
  TOP_K: 40, // Top-k for quality variety
  // Fallback settings
  FALLBACK_SUMMARY_LENGTH: 300, // Length for fallback summaries
  TRUNCATE_BUFFER: 20, // Buffer space for truncation
} as const;

// Dynamic chunk size calculation based on model context window
function calculateOptimalChunkSize(modelContextWindow?: number): number {
  if (!modelContextWindow) {
    return CONTEXT_CONFIG.CHUNK_SIZE;
  }

  // Adjust chunk size based on context window
  // Larger context windows can handle bigger chunks
  if (modelContextWindow >= 200000) { // Claude 3.5/3.7, GPT-4o
    return Math.min(CONTEXT_CONFIG.MAX_CHUNK_SIZE, 25);
  } else if (modelContextWindow >= 128000) { // GPT-4 Turbo
    return Math.min(CONTEXT_CONFIG.MAX_CHUNK_SIZE, 22);
  } else if (modelContextWindow >= 32000) { // GPT-4, Gemini 2.5
    return Math.min(CONTEXT_CONFIG.MAX_CHUNK_SIZE, 18);
  } else if (modelContextWindow >= 16000) { // GPT-3.5 Turbo
    return Math.min(CONTEXT_CONFIG.MAX_CHUNK_SIZE, 15);
  } else {
    return Math.max(CONTEXT_CONFIG.MIN_CHUNK_SIZE, 12);
  }
}

async function processChunksWithStoredSummaries(
  ctx: ActionCtx,
  conversationId: string,
  allMessages: ApiMessageDoc[]
): Promise<ProcessedChunk[]> {
  // Query for existing summaries for this conversation
  const existingSummaries = await ctx.runQuery(
    api.conversationSummary.getConversationSummaries,
    {
      conversationId: conversationId as any,
    }
  );

  log.info(`Found ${existingSummaries.length} existing summaries for conversation ${conversationId}`);

  // Create a map of chunk indices to their summaries
  const summaryMap = new Map<number, string>();
  for (const summary of existingSummaries) {
    summaryMap.set(summary.chunkIndex, summary.summary);
  }

  const optimalChunkSize = calculateOptimalChunkSize();
  const chunks: ProcessedChunk[] = [];

  // Process all messages in chunks
  for (let i = 0; i < allMessages.length; i += optimalChunkSize) {
    const chunk = allMessages.slice(i, Math.min(i + optimalChunkSize, allMessages.length));
    const chunkIndex = Math.floor(i / optimalChunkSize);
    
    // Check if we have a stored summary for this chunk
    const storedSummary = summaryMap.get(chunkIndex);
    
    if (storedSummary) {
      // Use the stored summary instead of raw messages
      chunks.push({
        summary: storedSummary,
        chunkIndex,
        originalMessageCount: chunk.length,
      });
      log.info(`Using stored summary for chunk ${chunkIndex} (${chunk.length} messages)`);
    } else {
      // No stored summary - use raw messages (will be summarized later if needed)
      chunks.push({
        messages: chunk,
        chunkIndex,
        originalMessageCount: chunk.length,
      });
      log.info(`Using raw messages for chunk ${chunkIndex} (${chunk.length} messages)`);
    }
  }

  return chunks;
}

// Extract summary storage logic into separate function
async function storeChunkSummary(
  ctx: ActionCtx,
  conversationId: string,
  chunkIndex: number,
  summary: string,
  messageCount: number,
  chunk: ProcessedChunk
): Promise<void> {
  try {
    // Calculate the required message IDs from the chunk
    const firstMessageId = chunk.messages?.[0]?._id || "unknown" as any;
    const lastMessageId = chunk.messages?.[chunk.messages.length - 1]?._id || "unknown" as any;
    
    await ctx.runMutation(api.conversationSummary.storeChunkSummary, {
      conversationId: conversationId as any,
      chunkIndex,
      summary,
      messageCount,
      firstMessageId,
      lastMessageId,
    });
    log.info(`Stored summary for conversation ${conversationId}, chunk ${chunkIndex}`);
  } catch (error) {
    log.warn(`Failed to store chunk summary: ${error}`);
    // Don't throw - continue with the process even if storage fails
  }
}

// Extract recursive meta-summary creation into separate function
async function createRecursiveMetaSummary(
  ctx: ActionCtx,
  conversationId: string,
  summaries: string[],
  depth: number = 1
): Promise<ProcessedChunk[]> {
  if (summaries.length <= CONTEXT_CONFIG.MAX_SUMMARY_CHUNKS) {
    return summaries.map(summary => ({ summary }));
  }

  log.info(`Creating meta-summary at depth ${depth} from ${summaries.length} summaries`);
  
  const metaSummaryChunks: ProcessedChunk[] = [];
  
  for (let i = 0; i < summaries.length; i += CONTEXT_CONFIG.MAX_SUMMARY_CHUNKS) {
    const summaryGroup = summaries.slice(i, Math.min(i + CONTEXT_CONFIG.MAX_SUMMARY_CHUNKS, summaries.length));
    const combinedSummaries = summaryGroup.join("\n\n");
    
    const metaSummaryPrompt = buildMetaSummaryPrompt(combinedSummaries, CONTEXT_CONFIG.MAX_SUMMARY_LENGTH);
    const metaSummary = await generateLLMSummary(metaSummaryPrompt, CONTEXT_CONFIG.MAX_SUMMARY_LENGTH);
    
    metaSummaryChunks.push({
      summary: metaSummary,
      isMetaSummary: true,
      chunkIndex: Math.floor(i / CONTEXT_CONFIG.MAX_SUMMARY_CHUNKS),
      originalMessageCount: summaryGroup.length,
    });
  }

  // Recursively create meta-summaries if we still have too many
  if (metaSummaryChunks.length > CONTEXT_CONFIG.MAX_SUMMARY_CHUNKS) {
    const summariesForNextLevel = metaSummaryChunks.map(chunk => chunk.summary!);
    return createRecursiveMetaSummary(ctx, conversationId, summariesForNextLevel, depth + 1);
  }

  return metaSummaryChunks;
}

async function summarizeChunk(chunk: ApiMessageDoc[]): Promise<string> {
  try {
    const conversationText = buildConversationText(chunk);
    const maxLength = Math.max(
      CONTEXT_CONFIG.FALLBACK_SUMMARY_LENGTH,
      Math.floor(conversationText.length * 0.25)
    );
    
    const summaryPrompt = buildSummaryPrompt(conversationText, maxLength);
    const summary = await generateLLMSummary(summaryPrompt, maxLength);
    
    log.info(`Generated LLM summary: ${summary.length} chars from ${conversationText.length} chars`);
    return summary;
  } catch (error) {
    log.warn(`LLM summarization failed: ${error}. Using fallback.`);
    const conversationText = buildConversationText(chunk);
    return createFallbackSummary(conversationText, CONTEXT_CONFIG.FALLBACK_SUMMARY_LENGTH);
  }
}

// Extract conversation text building into separate function
function buildConversationText(chunk: ApiMessageDoc[]): string {
  return chunk
    .map(msg => {
      const roleName = msg.role === "user" ? "User" : "Assistant";
      return `${roleName}: ${msg.content}`;
    })
    .join("\n\n");
}

// Extract summary prompt building into separate function
function buildSummaryPrompt(conversationText: string, maxLength: number): string {
  return `Please create a comprehensive, structured summary of this conversation segment. Preserve all key information, technical details, questions asked, solutions provided, and important context. Use clear formatting with sections or bullet points where appropriate.

Important guidelines:
- Maintain technical accuracy and preserve domain-specific terminology
- Capture the logical flow and progression of ideas
- Include both questions and answers/solutions
- Preserve specific examples, code snippets, or technical details
- Use structured formatting (sections, bullet points) for clarity
- Aim for ${maxLength} characters or less
- Focus on information that would be valuable for conversation continuity

Conversation to summarize:
${conversationText}

Structured Summary:`;
}

// Extract LLM summary generation into separate function
async function generateLLMSummary(prompt: string, maxLength: number): Promise<string> {
  try {
    // This would call your LLM API - for now using a placeholder
    // In real implementation, you'd call your chosen LLM API here
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": process.env.ANTHROPIC_API_KEY || "",
        "anthropic-version": "2023-06-01",
      },
      body: JSON.stringify({
        model: "claude-3-sonnet-20240229",
        max_tokens: Math.min(CONTEXT_CONFIG.MAX_API_TOKENS, maxLength * 2),
        temperature: CONTEXT_CONFIG.TEMPERATURE,
        top_p: CONTEXT_CONFIG.TOP_P,
        top_k: CONTEXT_CONFIG.TOP_K,
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
      }),
    });

    if (!response.ok) {
      throw new ConvexError(`LLM API error: ${response.status}`);
    }

    const data = await response.json();
    let summary = data.content?.[0]?.text || "";
    
    // Ensure summary doesn't exceed max length
    if (summary.length > maxLength) {
      summary = intelligentTruncateSummary(summary, maxLength);
    }
    
    return summary;
  } catch (error) {
    log.error(`LLM summarization failed: ${error}`);
    throw error;
  }
}

// Build a prompt for summarizing previously generated chunk summaries into a single coherent summary
function buildMetaSummaryPrompt(summaries: string, maxLength: number): string {
  return `Please create a comprehensive meta-summary that consolidates the following conversation summaries into a single, coherent overview. Preserve the most important information, maintain logical flow, and ensure no critical context is lost.

Guidelines:
- Combine related topics and themes
- Maintain chronological flow where important
- Preserve technical details and specific examples
- Keep important questions and their solutions
- Use structured formatting for clarity
- Aim for ${maxLength} characters or less
- Focus on information most valuable for conversation continuity

Summaries to consolidate:
${summaries}

Consolidated Meta-Summary:`;
}

function createFallbackSummary(text: string, maxLength: number): string {
  if (text.length <= maxLength) {
    return text;
  }

  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
  
  let summary = "";
  for (const sentence of sentences) {
    const trimmedSentence = sentence.trim();
    if (summary.length + trimmedSentence.length + 2 > maxLength - CONTEXT_CONFIG.TRUNCATE_BUFFER) {
      break;
    }
    summary += (summary ? ". " : "") + trimmedSentence;
  }
  
  if (summary.length > maxLength) {
    summary = summary.substring(0, maxLength - 3) + "...";
  }
  
  return summary || text.substring(0, maxLength - 3) + "...";
}

function intelligentTruncateSummary(summary: string, maxLength: number): string {
  if (summary.length <= maxLength) {
    return summary;
  }

  // Try to truncate at sentence boundaries
  const sentences = summary.split(/[.!?]+/);
  let truncated = "";
  
  for (let i = 0; i < sentences.length; i++) {
    const sentence = sentences[i].trim();
    if (!sentence) continue;
    
    const nextLength = truncated.length + (truncated ? ". " : "").length + sentence.length;
    if (nextLength > maxLength - CONTEXT_CONFIG.TRUNCATE_BUFFER) {
      break;
    }
    
    truncated += (truncated ? ". " : "") + sentence;
  }
  
  // If we couldn't fit even one sentence, do a hard truncate
  if (!truncated) {
    truncated = summary.substring(0, maxLength - 3) + "...";
  } else if (truncated.length < summary.length) {
    truncated += "...";
  }
  
  return truncated;
}

export {
  calculateOptimalChunkSize,
  processChunksWithStoredSummaries,
  storeChunkSummary,
  createRecursiveMetaSummary,
  summarizeChunk,
  buildConversationText,
  buildSummaryPrompt,
  generateLLMSummary,
  buildMetaSummaryPrompt,
  createFallbackSummary,
  intelligentTruncateSummary,
};